<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head>
<link href='https://fonts.googleapis.com/css?family=Source+Serif+Pro' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Source Serif Pro', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Fira+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Fira Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Eczar' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Eczar', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Space+Mono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Space Mono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Libre+Franklin' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Libre Franklin', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Cormorant+Garamond' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Cormorant Garamond', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Work+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Work Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Prociono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Prociono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=PT+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'PT Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Prociono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Prociono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=PT+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'PT Sans', sans-serif;}</style>
           <title>List of Figures</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 135--><div class="crosslinks"><p class="noindent">[<a 
href="mainch1.html" >next</a>] [<a 
href="mainli5.html" >prev</a>] [<a 
href="mainli5.html#tailmainli5.html" >prev-tail</a>] [<a 
href="#tailmainli6.html">tail</a>] [<a 
href="main3.html#mainli6.html" >up</a>] </p></div>
   <h2 class="likechapterHead"><a 
 id="x9-6000"></a>List of Figures</h2>
   <div class="tableofcontents"><span class="lofToc" >1.1&#x00A0;<a 
href="mainse2.html#x12-130011">Coordinate Reporting: coordinates in the MNI or Talaraich standard brain spaces
are typically reported in a tabular format in manuscripts [<span 
class="ec-lmbx-10">? </span>].</a></span><br /><span class="lofToc" >1.2&#x00A0;<a 
href="mainse2.html#x12-200012">Figure 1 from Salimi
et. al. that illustrates a 4-study, 1-dimensional meta-analysis. &#8220;A true signal (dashed
line) is created and four simulated statistic &#8216;images&#8217; are created by adding smoothed
white noise to the true signal (bold lines in the first column of the first four rows). To
apply CBMA to these simulated 1D studies, local maxima (foci) are extracted from each
observed signal (circles on the bold lines). Next, the locations of these foci are fed into
each CBMA technique. In the last row, the results of each method in reproducing the true
signal using the foci are shown. As can be seen, averaging over the complete signals (as
IBMA does) yields a better estimate of truth compared to using local maxima (CBMA).
ALE results in a smooth estimate (due to its Gaussian kernel) which KDA and MKDA
are rougher (due to its spherical kernel). Note that the &#8216;true&#8217; profile is generated as a
sum of Gaussian densities, which is most consistent with the ALE method.&#8221;</a></span><br /><span class="lofToc" >2.1&#x00A0;<a 
href="mainse7.html#x18-370011">Data
generation and analysis process. A subset of 465 datasets from the Human Connectome
Project (subjects) is used to generate 47 contrast maps (group maps) for each of groups
A and B for 500 subsamples. Within each subsample, an unthresholded image from A is
compared with each thresholded image from B with a particular similarity metric and
comparison strategy applied. Each image from A is then assigned the predicted class for
the max. arg from the set of B, and accuracy is calculated for the subsample.</a></span><br /><span class="lofToc" >2.2&#x00A0;<a 
href="mainse8.html#x19-400012">The size
of the mask across different thresholds for complete case analysis (cca) and single value
imputation (svi).</a></span><br /><span class="lofToc" >2.3&#x00A0;<a 
href="mainse8.html#x19-400023">Accuracy of image contrast classification at varying levels of image
thresholding, for comparison of an unthresholded image against a set of images at each
threshold, including positive and negative values (left) and positive values only (right).
Complete case analysis (CCA) with a Pearson score had a maximum accuracy of 0.984
for a threshold of Z = +/- 1.0 (0.983, 0.985), outperforming single-value imputation
(SVI).</a></span><br /><span class="lofToc" >2.4&#x00A0;<a 
href="mainse8.html#x19-410014">Accuracy of image contrast classification at varying levels of thresholding, for
the worst performing image, "0-back body," from the working memory task. Accuracy
peaked at a threshold of Z = +/- 1.0 for complete case analysis with a Pearson score and
                                                                                   

                                                                                   
at a threshold of Z = + 2.0 for complete case analysis with a Spearman score for each of
positive and negative values (left) and positive values only (right).</a></span><br /><span class="lofToc" >2.5&#x00A0;<a 
href="mainse8.html#x19-420015">Mean accuracy
+/- 1.0 standard deviation for each contrast across 500 random folds for the optimally
performing threshold (Z = +/- 1.0), direction (positive and negative), comparison strategy
(complete case analysis) and similarity metric (Pearson score). Interactive confusion
matrices for all thresholds, comparison strategies, and similarity metrics are available
(http://vsoch.github.io/image-comparison-thresholding) </a></span><br /><span class="lofToc" >3.1&#x00A0;<a 
href="mainse12.html#x24-490011">Overview of the approach.
Group statistical brain maps annotated with Cognitive Atlas concepts come from the
NeuroVault database (top left), and concept relationships are represented in the Cognitive
Atlas (bottom right). Spatial similarity is compared to purely semantic similarity using
representational similarity analysis (top right), and a classification encoding model is
used to test the predictive ability of semantic annotations (bottom right). An encoding
model is generated at each voxel to predict a vector of 93 image voxels (Y) using concept
labels (X) excluding two held out images, generating a vector of 132 coefficients (R)
for each of v voxels. These vectors can be assembled into a matrix (R) to generate a
predicted image (Yp) using a held out concept vector (Xn). </a></span><br /><span class="lofToc" >3.2&#x00A0;<a 
href="mainse13.html#x25-670012">Concepts associated
with correct and incorrect predictions. Regression parameters from the encoding model
reconstructed into brain maps to provide a visual understanding of what the model
learned, for concepts consistently associated with correct predictions (top row), and
associated with incorrect predictions (bottom row). Maps used in the classification task
included voxelwise data, and are thresholded above to show the highest positive (red)
and negative (blue) values.</a></span><br /><span class="lofToc" >3.3&#x00A0;<a 
href="mainse13.html#x25-690013">Comparison of semantic to spatial similarity. Semantic
image comparison is correlated with spatial image comparison (pearson score of 0.514
(95% confidence interval = (0.499,0.53), p-value &#x003C; 2.2e-16)) across all pairwise image
comparisons. Task and contrast specific RSA scores are included in Supplementary data
3.2.</a></span><br /><span class="lofToc" >A.1&#x00A0;<a 
href="mainse18.html#x32-890011">The Experiment Factory Core includes Experiments, a battery skeleton, and
the software, all of which are openly available on Github. Installation of the Experiment
Factory tool allows a researcher to run a sequence of experiments on the fly (right panel) or
to generate a local folder or virtual machine to deploy experiments to Amazon Mechanical
Turk using Psiturk (left panel).</a></span><br /><span class="lofToc" >A.2&#x00A0;<a 
href="mainse18.html#x32-930012">Expfactory-docker includes the main application
container (expfactory), a database for storing application data (postgres), a job queue
(redis) and worker (Celery) for running computationally intensive tasks, and a web server
(nginx) to serve the application to the web. </a></span><br /><span class="lofToc" >B.1&#x00A0;<a 
href="mainse22.html#x37-1120011">Functional connectivity visualizations:
Trends in functional connectivity visualizations have changed over time, including A)
the orthogonal view, showing an axial, coronal, and sagittal slice of a functional map
overlayed on an anatomical brain, B: graph based network approaches that model regions
as nodes and links as hubs, C) equivalent networks in the context of brain space, D) force
                                                                                   

                                                                                   
directed graphs that represent connectivity with distance, and E) bundling techniques to
group clusters of connections. </a></span><br /><span class="lofToc" >C.1&#x00A0;<a 
href="mainse27.html#x43-1510011">The MyConnectome virtual machine was a successful
strategy to completely reproduce a complicated analysis involving brain imaging, genomics,
behavior, and metabolomic data.</a></span><br /><span class="lofToc" >C.2&#x00A0;<a 
href="mainse28.html#x44-1540012">The Pybraincompare Python package renders the
interactive search interface to power spatial image comparison in the NeuroVault database.</a></span><br />
   </div>
                                                                                   

                                                                                   
                                                                                   

                                                                                   
   <!--l. 137--><div class="crosslinks"><p class="noindent">[<a 
href="mainch1.html" >next</a>] [<a 
href="mainli5.html" >prev</a>] [<a 
href="mainli5.html#tailmainli5.html" >prev-tail</a>] [<a 
href="mainli6.html" >front</a>] [<a 
href="main3.html#mainli6.html" >up</a>] </p></div>
<!--l. 137--><p class="indent" >   <a 
 id="tailmainli6.html"></a>  
</body></html> 
