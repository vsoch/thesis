<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head>
<link href='https://fonts.googleapis.com/css?family=Source+Serif+Pro' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Source Serif Pro', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Fira+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Fira Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Eczar' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Eczar', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Space+Mono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Space Mono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Libre+Franklin' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Libre Franklin', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Cormorant+Garamond' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Cormorant Garamond', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Work+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Work Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Prociono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Prociono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=PT+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'PT Sans', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=Prociono' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'Prociono', sans-serif;}</style>
           
<link href='https://fonts.googleapis.com/css?family=PT+Sans' rel='stylesheet' type='text/css'>
              <style>body {font-family: 'PT Sans', sans-serif;}</style>
           <title>Methods</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 1171--><div class="crosslinks"><p class="noindent">[<a 
href="mainse13.html" >next</a>] [<a 
href="mainse11.html" >prev</a>] [<a 
href="mainse11.html#tailmainse11.html" >prev-tail</a>] [<a 
href="#tailmainse12.html">tail</a>] [<a 
href="mainch3.html#mainse12.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">3.2   </span> <a 
 id="x24-480003.2"></a>Methods</h3>
<!--l. 1173--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2.1   </span> <a 
 id="x24-490003.2.1"></a>Overview of Open Infrastructure</h4>
<!--l. 1175--><p class="noindent" >This work uses completely openly available data (accessible via API) and methods (publicly released
packages) for performing all analyses and visualization. I first select a set of publicly available brain maps
from the NeuroVault database, and annotate these maps with terms defined in the Cognitive Atlas to
generate a concept graph describing concepts, tasks, and their relationships. By way of recently released
infrastructure to tag images with contrasts from the Cognitive Atlas in NeuroVault, images are
immediately mapped to this tree under the concepts they represent, allowing for a completely
semantic-based image comparison analysis. I test the predictive ability of semantic annotations in a
classification framework (Section 3.2.3 Semantic image comparison in a classification framework), and show
value in semantic image comparison by comparing to the current standard, spatial similarity (Section 3.2.5
Comparison of semantic to spatial similarity), and then. An overview of the approach is provided
in Figure&#x00A0;<a 
href="#x24-490011">3.1<!--tex4ht:ref: fig:31 --></a>, scripts with robust documentation to reproduce all analysis are available [<span 
class="ec-lmbx-10">?</span>
].
<!--l. 1192--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                                   

                                                                                   
<a 
 id="x24-490011"></a>
                                                                                   

                                                                                   
<div class="center" 
>
<!--l. 1193--><p class="noindent" >

<!--l. 1194--><p class="noindent" ><img 
src="images/figure31.png" alt="PIC"  
></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3.1: </span><span  
class="content"> Overview of the approach. Group statistical brain maps annotated with Cognitive Atlas
concepts come from the NeuroVault database (top left), and concept relationships are represented
in the Cognitive Atlas (bottom right). Spatial similarity is compared to purely semantic similarity
using representational similarity analysis (top right), and a classification encoding model is used to
test the predictive ability of semantic annotations (bottom right). An encoding model is generated
at each voxel to predict a vector of 93 image voxels (Y) using concept labels (X) excluding two held
out images, generating a vector of 132 coefficients (R) for each of v voxels. These vectors can be
assembled into a matrix (R) to generate a predicted image (Yp) using a held out concept vector
(Xn). <br 
class="newline" /><br 
class="newline" /></span></div><!--tex4ht:label?: x24-490011 -->
                                                                                   

                                                                                   
<!--l. 1209--><p class="indent" >   </div><hr class="endfigure">
   <h4 class="subsectionHead"><span class="titlemark">3.2.2   </span> <a 
 id="x24-500003.2.2"></a>Data and Curation</h4>
<!--l. 1214--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-510003.2.2"></a>NeuroVault</h5>
<!--l. 1216--><p class="noindent" >The NeuroVault database is a valuable resource for the sharing of statistical brain maps, each the result of
an experiment to measure a cognitive process, as well as tools to annotate these maps. Researchers
are incentivized to upload their maps here either for purpose of data sharing (such as for a
journal requirement), visualization of maps, or as an easy way to archive a result for long term
reproducibility. To prepare NeuroVault for this application, I first added the ability of NeuroVault
users to tag brain maps with contrast labels from tasks defined in the Cognitive Atlas [<span 
class="ec-lmbx-10">? </span>].
To select datasets, I filtered images to include those with a DOI, in MNI space, and which
were&#x00A0;unthresholded. This resulted in 93 statistical brain maps across 22 collections, including
both Z and T maps. I use a method described previously [<span 
class="ec-lmbx-10">? </span>] to convert T score maps to Z
score maps, obtaining the number of subjects, N, in each study to calculate the degrees of
freedom (N-2) using the NeuroVault API. All maps are resampled to a MNI 2mm standard
brain.
<!--l. 1234--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-520003.2.2"></a>Cognitive Atlas</h5>
<!--l. 1236--><p class="noindent" >The Cognitive Atlas is a community effort to define tasks and associated concepts studied in cognitive
neuroscience, and the relationships between them. As an ontology, it maps nicely onto a graph framework
in that entities (concepts and tasks) are represented by nodes in a graph, and the relationships between
them are the connections. By describing tasks in the language of concepts that encompass these nodes, and
mapping brain image results to this tree, I can walk along the branches to understand how something like
&#8220;theory of mind&#8221; relates to &#8220;anxiety,&#8221; and perform computations on the graph to describe the universe of
brain imaging results in this context. Toward this goal, we&#x00A0;first annotated contrasts from experimental
paradigm with the concepts they measure,&#x00A0;and then defined relationships between them, discussed
next.
<!--l. 1250--><p class="noindent" ><span class="paragraphHead"><a 
 id="x24-530003.2.2"></a><span 
class="ec-lmbx-10">Concept association with contrasts</span></span>
   it is only possible to&#x00A0;make inferences about how a concept like &#8220;anxiety&#8221; is represented by a particular
statistical brain map result given an understanding about the cognitive concepts that are queried by a large
                                                                                   

                                                                                   
set of experiments. This information, the relationship between a task&#8217;s contrast (e.g., &#8220;angry faces minus
baseline&#8221;) and the measured concepts (e.g., &#8220;anxiety&#8221;), is called an &#8220;assertion&#8221; and these assertions make
up the relationships of the Cognitive Atlas.
<!--l. 1260--><p class="indent" >   Toward this goal, the Poldrack Lab at Stanford University established a regularly meeting Cognitive
Atlas Annotation group that would review a set of contrast images publicly available in NeuroVault, and
generate a list of cognitive concepts queried by each one. This process included communication with
experts in the field, and care was taken to contact the owners of each dataset described in the NeuroVault
database to discuss the task and confirm tagged cognitive contrasts. I have made our process available,
including &#8220;best practices&#8221; for other researchers to do the same [<span 
class="ec-lmbx-10">? </span>]. This procedure provided a list of 132
concepts defined in the Cognitive Atlas across 29 cognitive paradigms that were represented by the 93
maps.
<!--l. 1272--><p class="noindent" ><span class="paragraphHead"><a 
 id="x24-540003.2.2"></a><span 
class="ec-lmbx-10">Definition of relationships in Cognitive Atlas</span></span>
   One of the strengths of an ontology like the Cognitive Atlas comes by way of making assertions that
entities, in this case the cognitive concepts, are meaningfully related. &#x00A0;These relationships are the links in
the graph, defined by the OBO Relations Ontology [<span 
class="ec-lmbx-10">? </span>], of which the Cognitive Atlas uses
&#8220;is a&#8221; and &#8220;part of.&#8221; For example, the &#8220;part of&#8221; relationship formally states, &#8220;Everything is
part of itself. Any part of any part of a thing is itself part of that thing. Two distinct things
cannot be part of each other.&#8221; Applied to concepts in the Cognitive Atlas, an example would be
&#8220;emotion regulation&#8221; is a part of &#8220;emotional expression,&#8221; and this assertion was made because
regulation of emotion is an essential component for the expression of an emotion. I represent these
relationships as different weights attributed to links in the graph. I made an effort to contact 35
&#8220;experts&#8221; in cognitive psychology to elicit contribution to the atlas, however participation was not
substantial enough to propose as the sole method for defining such relationships. I selected a
subset of the ontology under the category of &#8220;Reasoning and Decision Making&#8221; for which the
Poldrack Lab has expertise [<span 
class="ec-lmbx-10">? ? ? ? ? ? ? ? ? ? ? </span>], and spent many months going through each
concept to review both the assertions (concept &#8220;is a kind of&#8221; and &#8220;is a part of&#8221;) to ensure that
the tree was not sparse. I believe that the group expertise, combined with expert opinion of
collaborators, produces a graph that is suitable to describe relationships between these 93
maps.
<!--l. 1299--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2.3   </span> <a 
 id="x24-550003.2.3"></a>Semantic image comparison in a classification framework</h4>
<!--l. 1300--><p class="noindent" >To test the predictive value of semantic annotations, I employ an encoding model that generated a
predicted brain image based on a binary vector of concept labels, and used a leave-two out classification
method to assess accuracy. To generate an encoding model, I train a regularized linear classifier, the elastic
                                                                                   

                                                                                   
net [<span 
class="ec-lmbx-10">? </span>] at each voxel to generate sparse regression coefficients for each concept that can be multiplied
by a new concept vector to generate the predicted image. This approach will be applied to
two datasets to demonstrate its validity: first in the context of whole brain statistical maps
(IBMA) using the 93 result contrast images from the NeuroVault database, and second in
the context of coordinate-based statistical maps (CBMA) using the NeuroSynth database ([<span 
class="ec-lmbx-10">?</span>
]).
<!--l. 1310--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-560003.2.3"></a>Prediction of whole brain statistical maps using cognitive concepts</h5>
<!--l. 1311--><p class="noindent" >This analysis aims to demonstrate the utility of semantic annotations of whole brain statistical maps to
predict whole brain statistical maps. Specifically, we want to show that a vector of binary concept labels for
a held out image can be used to generate a predicted image with a pattern of spatial activation consistent
with the actual image. I define the training data, X as a 93 by 132 matrix of images by concepts, where
each value is a binary indicator if the image is tagged with the concept. I define the Y as a vector of 93
voxel values, one for each image, and by building a model at each voxel to output a vector of 132
coefficients, I can assemble this into a regression parameter matrix R of size 28,549 (voxels) by 132
(concepts). The assessment of 28,549 voxels is due to the need to resample the original 2mm brain maps
to 4mm in order to make the analysis computationally tractable. I can then multiple a new
image, N, represented by a binary concept vector of 132 by 1 to generate a predicted image of
size 28,549 by 1.&#x00A0;An illustration of this procedure is provided in the bottom right panel of
Figure&#x00A0;<a 
href="#x24-490011">3.1<!--tex4ht:ref: fig:31 --></a>.
<!--l. 1323--><p class="indent" >   Using the encoding procedure detailed above, for each set of pairwise images in the set, I performed the
following procedure to generate predictions for two held-out images: <br 
class="newline" />
<!--l. 1327--><p class="indent" >   <span 
class="ec-lmbx-10">for image1 in all images: </span><br 
class="newline" />      <span 
class="ec-lmbx-10">for image2 in all images </span><br 
class="newline" />         if image1 != image2 and contrast of image1 != contrast of image2:<br 
class="newline" />           hold out image1 and image2, build encoding model using other <br 
class="newline" />           generated predicted image PR1 for image1 <br 
class="newline" />           generated predicted image PR2 for image2 <br 
class="newline" />           classify image1 as being more similar to PR1 or PR2 using spatial similarity
<br 
class="newline" />           classify image2 as being more similar to PR1 or PR2 using spatial similarity
<br 
class="newline" />           assign correct classification if image1 is more similar to PR1 <br 
class="newline" />           assign correct classification if image2 is more similar to PR2 <br 
class="newline" />
                                                                                   

                                                                                   
<!--l. 1342--><p class="indent" >   I do not include images in the assessment with equivalent contrast labels, for a final potential
comparison set of N=8556. To assess spatial similarity I use the same complete case analysis procedure
discussed in 2.2.2. Running this procedure for all unique pairwise images will result in two
predictions per set, each of which can be correct or incorrect, for a total of 17,112 predictions. To
calculate an overall accuracy, I calculate the number correct divided by the total predictions:
<br 
class="newline" />
<!--l. 1351--><p class="indent" >   &#x00A0; &#x00A0; &#x00A0; accuracy = <span class="underline">number correct</span>
<!--l. 1353--><p class="indent" >   &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0;total predictions
<!--l. 1355--><p class="noindent" ><span class="paragraphHead"><a 
 id="x24-570003.2.3"></a><span 
class="ec-lmbx-10">Classifier Validation</span></span>
   To assess if this accuracy value is significantly different from chance, I perform the procedure for
prediction of whole brain statistical maps as previously explained 1000&#x00A0;times with&#x00A0;~1046 image sets (a
quarter of all total image sets, chosen based on computational resources available), but randomly shuffling
the image labels to generate a null distribution of 1000 accuracies. I can then calculate a p-value to assess
where the actual accuracy falls in this distribution, and assess if the actual accuracy is significantly
different from this null distribution of accuracies that represent the classification procedure under random
chance. Finally, I&#x00A0;generate a confusion matrix of images by images to assess which images are commonly
mistaken for other images.
<!--l. 1368--><p class="noindent" ><span class="paragraphHead"><a 
 id="x24-580003.2.3"></a><span 
class="ec-lmbx-10">Learned Concept Map Validation</span></span>
   Assessing the frequency for which each cognitive concept is associated with correct or incorrect
classification gives insight to the quality of the cognitive concepts themselves. Toward this goal, I generated
a concept confusion matrix, where each row is a single concept from the Cognitive Atlas, and the
columns are counts for the number of times the concept was associated with a correct prediction
(&#8220;correct&#8221;), and the number of times a concept was associated with an incorrect prediction
(&#8220;incorrect&#8221;). Given that brain maps in the NeuroVault database are organized by cognitive paradigm
(task) and collection (a group of images uploaded by a researcher), I would also be interested in
knowing the frequency at which an incorrect prediction (all values of the matrix not in the
diagonal) could be labeled as a within-collection (between-task) confusion, a between-task
confusion, or between-collection confusion. I can generate these values by assigning each box in the
confusion matrix to one of these categories depending on the relationship between the two
images, and then summing the confusion values for each category, and dividing by the total
number of confusion values. This procedure will produce one value per category representing the
percentage of incorrect predictions that are within-collection (between-task), between-task, or
between-collection.
                                                                                   

                                                                                   
<!--l. 1389--><p class="indent" >   Finally, as each concept is associated with a vector of voxels, one for each in a brain map, I can
reconstruct the vector into a standardized (Z-scored) brain map to visualize the coefficient learned at each
model. This is a form of soft validation that will allow us to view what the model has learned about each
cognitive concept from a set of annotated images.
<!--l. 1396--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-590003.2.3"></a>Prediction of coordinate-based statistical brain maps using cognitive concepts</h5>
<!--l. 1397--><p class="noindent" >The previous analysis is suited to demonstrate the value of semantic annotations in the context of
whole brain statistical maps, and this next analysis aims to show that semantic annotations
also have predictive value in the context of coordinate-based data. The NeuroSynth database
([<span 
class="ec-lmbx-10">? </span>]) is a well-established, automatically derived database that associated x,y,z coordinate
reports in papers with terms from the abstract. At the time of this analysis, the database
contained 11,405 abstracts, each associated with a set of coordinates. As previously described,
NeuroSynth provides, for each abstract, a normalized value to reflect the prevalence of each
term within an abstract, and uses these associations to derive the probability of activation
given a term (feature) given a set of input abstracts (Section 1.2.1). As I am interested in a
specific set of cognitive concept terms from the Cognitive Atlas, many not represented in the
NeuroSynth term set, I obtained the original set of 11,405 abstracts, and re-generated these
normalized tables using the same procedure defined in [<span 
class="ec-lmbx-10">? </span>]. Terms were filtered to include only
those present in 10 or more abstracts. This procedure produced a table of abstracts by 399
terms, a number that is substantially larger than the 132 from the previous analysis, as we are
not limited to the terms annotated via the 93 images. This matrix, Y, of size 11,405 by 399,
contains the annotations for each abstract that will be used to predict the coordinate-based brain
maps.
<!--l. 1399--><p class="indent" >   To generate the coordinate-based maps, I resample the binary coordinate data (a 1 indicates the
coordinate is reported in the paper associated with an abstract) to a 4mm space, and this provides a sparse
brain image associated with each abstract. I next use this data in the equivalent context as the 93
NeuroVault images: we want to show that the vector of 399 concept labels can be used to predict the
coordinate data. We employ the equivalent classification framework, however instead of holding out two
images to make two predictions per iteration, we hold out approximately 1% of abstracts at
each iteration, building the encoding model from the other 99%. We can then, for all pairwise
images in the holdout set, use the trained model to generate the predicted images, and test
whether the similarity is higher between the true and predicted for each pair. The calculation of
accuracy is equivalent to as before, a ratio between the number of correct predictions out of all
predictions.
                                                                                   

                                                                                   
<!--l. 1402--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2.4   </span> <a 
 id="x24-600003.2.4"></a>Comparison of semantic to spatial similarity</h4>
<!--l. 1404--><p class="noindent" >An assessment of semantic comparison of brain maps, independent of any operations on the spatially
defined values, would be valuable to compare against the standard of using a voxelwise metric. Toward this
goal, I first wanted to develop a graph-based metric for semantic image comparison that would derive
similarity from semantic annotations and relationships represented as weights in the graph. The cognitive
concepts are vertices in the graph, and each contrast image then is a set of vertices. First I will review
graph metrics to compare vertices, and then discuss application of these metrics to compare sets of
vertices.
<!--l. 1411--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-610003.2.4"></a>Semantic Comparison of Graphs</h5>
<!--l. 1412--><p class="noindent" >The majority of similarity metrics for graphs are based on comparing vertices. Specifically, well-known
methods include:
     <ul class="itemize1">
     <li class="itemize"><span 
class="ec-lmbx-10">ASCOS</span>: "Asymmetric network Structure COntext Similarity" infers the similarity between
     nodes based solely on structure context, i.e., the patterns of the edges, because nodes with
     similar structure are likely to have similar attributes. This algorithm is similar to a well-known
     algorithm called SimRank, however due to comparing all paths between nodes, ASCOS tends
     to outperform SimRank [<span 
class="ec-lmbx-10">? </span>]
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">COSINE</span>: If the i-th and j-th rows/columns of an adjecency matrix are regarded as two
     vectors, then we can calculate the cosine of the angle between them to derive a similarity
     measure. The cosine similarity of i and j is the number of common neighbors divided by the
     geometric mean of their degrees. [<span 
class="ec-lmbx-10">? </span>]
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">JACCARD</span>: calculates the number of common neighbors divided by number of vertices that
     are neighbors of at least one of the two vertices being considered. [<span 
class="ec-lmbx-10">? </span>]
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">KATZ</span>:  Katz  centrality  computes  the  relative  influence  of  a  node  within  a  network  by
     measuring the number of the immediate neighbors and other nodes that connect to the node
     via the immediate neighbors. Connections to distant neighbors are penalized by an attenuation
     factor [<span 
class="ec-lmbx-10">? </span>]
                                                                                   

                                                                                   
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">LHN</span>: "Leicht-Holme-Newman" takes after Jaccard, but "punishes" the high degree nodes even
     more [<span 
class="ec-lmbx-10">? </span>].
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">DICE</span>: is twice the number of common neighbors divided by the sum of the degrees of the
     vertices [<span 
class="ec-lmbx-10">? </span>]
     </li>
     <li class="itemize"><span 
class="ec-lmbx-10">INVERSE LOG WEIGHTED</span>: is the number of common neighbors weighted by the inverse
     logarithm of their degrees. The rationale for this approach is the idea that two vertices should
     be considered more similar if they share a low-degree common neighbor [<span 
class="ec-lmbx-10">? </span>].</li></ul>
<!--l. 1431--><p class="indent" >   The Cognitive Atlas concept space consists of cognitive concepts linked by relationship types "is a kind
of" and "is a part of," and simple graphs can be generated that represent links with one or both of these
edges. To calculate pairwise comparison of cognitive concepts, I <a 
href="https://github.com/vsoch/semantic-image-comparison/blob/master/analysis/graph/similarity.py" >implemented these metrics</a> and used
networkx [<span 
class="ec-lmbx-10">? </span>] to generate a graph to represent each of the "part of," and "kind of" relationships, and the two
combined. I could then use these concept similarity matrices to calculate similarity between image contrasts
by the following procedure for some square concept similarity matrix, df, and two sets of cognitive
concepts, C1 and C2: <br 
class="newline" />
<!--l. 1433--><p class="indent" >   Subset rows of df to C1 and sum across rows <br 
class="newline" />      Subset columns to C2 and sum across columns <br 
class="newline" />         Divide by number of concepts in set(C1,C2) <br 
class="newline" />
<!--l. 1437--><p class="indent" >   The above procedure produces a mean similarity score between the two sets of concepts for a contrast of
interest. To supplement these metrics, we can also include a well-known method to compare gene sets using
the Gene Ontology [<span 
class="ec-lmbx-10">? </span>]. While many methods from this space rely on having annotated databases [<span 
class="ec-lmbx-10">? </span>] or
finding enrichment of genes in some list [<span 
class="ec-lmbx-10">? </span>], Wang is ideal for this application because it does not need an
annotated database of terms or probabilities. I developed an R package [<span 
class="ec-lmbx-10">? </span>] that can take as input a
contrast defined in the Cognitive Atlas, and return such a score. The metric is based on Wang&#8217;s metric [<span 
class="ec-lmbx-10">? </span>],
which has had most wide application to compare gene sets. The metric aims to assess the semantic
contribution of ancestor terms. This means that for any two concepts, C1 and C2 in the graph, one can
walk up the tree and define a list of concept nodes, and associated &#8220;is a&#8221; and &#8220;part of&#8221; relationships
between these nodes, all the way to the highest (parent) base node of the ontology. The weight
for each concept is determined by multiplying the last (child node) weight by&#x00A0;0.8 for &#8220;is a&#8221;
relationships, and 0.6 for &#8220;part of&#8221; relationships, as implemented in the initial paper [<span 
class="ec-lmbx-10">? </span>], and I
believe to be a reasonable weighting to reflect similarity of a type of synonym (&#8220;is a&#8221;) or (&#8220;part
of&#8221;). The multiplication means that weights decrease as one moves up the tree toward the
                                                                                   

                                                                                   
root. I then take the intersection of concepts between C1 and C2, and the similarity score is
calculated as the ratio of intersected weights&#x00A0;to all the weights&#x00A0;defined between the two concepts.
<br 
class="newline" />
<!--l. 1457--><p class="indent" >   similarity = &#x00A0; &#x00A0; &#x00A0; <span class="underline">sum(intersected weights)</span>
<!--l. 1459--><p class="indent" >   &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; sum(all weights)<br 
class="newline" /><br 
class="newline" />
<!--l. 1461--><p class="indent" >   This procedure derives a single score to compare each pairwise concept defined in the ontology tree, and
forms a matrix that can be used for comparison to a spatial similarity.
<!--l. 1465--><p class="indent" >   One would have confidence that the graph method is capturing meaningful relationships between brain
maps if the semantic similarity matrices are comparable to a spatially derived equivalent matrix. Thus, for
the final step of validation I compared spatial and semantic similarity. Maps that query more similar
cognitive processes should have higher spatial similarity.
<!--l. 1472--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-620003.2.4"></a>Derivation of spatial similarity matrix</h5>
<!--l. 1474--><p class="noindent" >To assess the spatial similarity of maps, I used an approach from recently published work [<span 
class="ec-lmbx-10">? </span>] to derive
pairwise comparison of the 93 statistical brain maps maps using complete case analysis with a pearson
score. While this work found an optimal classification accuracy for image contrasts using a
threshold of Z=+/-1, I chose to use the unthresholded maps, as this is the transformation that was
used for the&#x00A0;semantic similarity assessment,&#x00A0;maximizes the comparison set between the two
images, and is not significantly different from a threshold of Z=+/-1 [<span 
class="ec-lmbx-10">? </span>]. The approach is as
follows:
<!--l. 1484--><p class="indent" >   For each of the 93 NeuroVault maps, N1: <br 
class="newline" />
<!--l. 1486--><p class="indent" >   &#x00A0; &#x00A0; For each of the 93 NeuroVault maps, N2:
<!--l. 1488--><p class="indent" >   &#x00A0; &#x00A0; &#x00A0; &#x00A0; generate a pairwise deletion mask (complete case analysis)
<!--l. 1490--><p class="indent" >   &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; &#x00A0; calculate pearson score<br 
class="newline" />
<!--l. 1492--><p class="indent" >   This procedure resulted in a matrix of spatial image comparisons to compare the semantic comparisons
to.
<!--l. 1495--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x24-630003.2.4"></a>Representational similarity analysis</h5>
                                                                                   

                                                                                   
<!--l. 1497--><p class="noindent" >Representation similarity analysis (RSA) is a simple method to compare the similarity of two matrices
based on a pearson correlation coefficient between corresponding upper triangles [<span 
class="ec-lmbx-10">? </span>]. I used RSA to make
an assessment of the semantic image comparison similarity matrix defined in 3.2.5 against a spatial image
comparison matrix defined in the previous section. This procedure can also be done for submatrices, for
example, filtering the matrix to images that are relevant to a specific cognitive concept or paradigm. These
scores provide an overall similarity of semantically and spatially derived image comparisons globally, as well
as within task and concept groups.
                                                                                   

                                                                                   
   <!--l. 1508--><div class="crosslinks"><p class="noindent">[<a 
href="mainse13.html" >next</a>] [<a 
href="mainse11.html" >prev</a>] [<a 
href="mainse11.html#tailmainse11.html" >prev-tail</a>] [<a 
href="mainse12.html" >front</a>] [<a 
href="mainch3.html#mainse12.html" >up</a>] </p></div>
<!--l. 1508--><p class="indent" >   <a 
 id="tailmainse12.html"></a>    
</body></html> 
